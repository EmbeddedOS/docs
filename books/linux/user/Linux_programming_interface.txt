Book: The Linux Programming Interface - A Linux and UNIX System Programming Handbook


4. File I/O: The universal I/O model 
	+ Files are a good place to start, since they are central to the UNIX philosophy. 
	
	4.1. Overview:
		+ All system calls for performing I/O refer to open files using a file descriptor, \
			a (usually small) non-negative in integer.
			
		+ File descriptors are used to refer to all types of open file, \
			including pipes, \
			FIFOs, \
			sockets, \
			terminals, \
			devices, \
			and regular files.
			
		+ Each process has its own set of file descriptor.
		
		+ By convention, most programs expect to be able to use THREE standard file descriptors listed in:
			File descriptor 	Purpose 			POSIX name 		stdio stream
			
			0 					standard input 		STDIN_FILENO 	stdin
			1 					standard output 	STDOUT_FILENO 	stdout
			2 					standard error 		STDERR_FILENO 	stderr
			
		+ When referring to these file descriptors in a program, \
			we can use either the number (0, 1 or 2) or, preferably, \
			the POSIX standard names defined in <unistd.h>.
			
		+ The following are four key system calls for performing file I/O, \
			(Programming languages and software packages typically employ these calls only indirectly, via I/O libraries):
			+ fd = open(pathname, flags, mode)
				+ Open the file identified by pathname.
				+ Return a file descriptor used to refer to the open file in subsequent calls.
				+ If the file doesn't exist, open() may create it, \
					depending on the settings of the flags bit-mask argument. \
					The flags argument also specifies whether the file is to be opened for reading, writing or both.
				+ The mode argument specifies the permissions to be placed on the file if it is created by this call. \
					If the open() call is not being used to create a file, this argument is ignored \
					and cam be omitted.
				
			+ num_read = read(fd, buffer, count)
				+ reads at most count bytes from the open file referred to by fd and stores them in buffer.
				+ The read() call returns the number of bytes actually read.
				
			+ num_write = write(fd, buffer, count)
				+ writes up to count byte from buffer to the open file referred to by fd.
				+ The write() call returns the number of bytes actually written, which may be less than count.
				
			+ status = close(fd)
				+ Is called after all I/O has been completed, \
					in order to release the file descriptor fd and its associated kernel resources.
					
	4.2. Universality of I/O:
		+ One of the distinguishing features of the UNIX I/O model \
			is the concept of universality of I/O.
			
		+ This means that the same four system calls: open(), read(), write(), and close() \
			-- are used to perform I/O on ALL TYPES OF FILES, \
			including devices such as terminals. 
			
		+ Consequently, if we write a program using only these system calls, \
			that program will work on ANY TYPE OF FILE.
			
		+ Universality of I/O is achieved by ensuring that each file system and \
			device driver implements the SAME set of I/O system calls.
			
		+ Because details specific to the file system 
			
			
			
			
11. System limits and options:
	+ Each UNIX implementation sets limit on various system features and resources, \
		And provides- or chooses not to provide- options defined in various standards. \
		Examples include the following:
		+ How many files can a process hold open at one time?
		+ Does the system support realtime signals?
		+ What is the largest value that can be stored in a variable of type int?
		+ How big an argument list can a program have?
		+ What is the maximum length of a pathname ?
		


12. System and Process information:
	+ In this chapter, We look at ways of accessing a variety of system and process information.
	+ The primary focus of the chapter is a discussion of the '/proc' file system.
	+ We also describe the uname() system call.
	
	12.1 The '/proc' file system:
		+ In older UNIX implementations, there was typically no easy way to change attributes of the kernel, \
			To answer questions such as the following:
			+ How may processes are running on the system and who owns them ?
			+ What files does a process have open ?
			+ What files are currently locked, and which processes hold the locks ?
			+ What sockets are being used on the system?
			
		+ Some older UNIX implementations solves this problem by allowing privileged programs to delve into data structures in kernel memory. \
			Howerver, this approach suffered various problems. \
			In particular, it required specialized knowledge of the kernel the data structures, \
			And these structures might change from one kernel version to the next, \
			Requiring programs that depended on them to be rewritten.
			
		+ In order to provide easier access to kernel information, \
			many modern UNIX implementations provide a '/proc' virtual file system. \
			This file system resides under thr '/proc' directory \
			and contains various files that expose kernel information, \
			Allow processes to conveniently read that information, \
			And change it in some cases using normal file I/O system calls.
			
		+ The /proc file system is said to be virtual \
			because the files anf subdirectories that it contains don't reside on the disk. \
			Instead, The kernel creates them "on the fly" as processes access them.
			
		
	12.2. Obtaining Information About a Process: '/proc/PID/'
		+ For each process on the system, the kernel provides a corresponding directory named procPID, \
			where PID is the ID of the process.
		+ Within this directory are various files and subdirectories containing information about that process. \
			For example, we can obtain information about the init process, \
			Which always has the process ID 1, by looking at files under the directory '/proc/1/'.
			
		+ 'status' file in each '/proc/PID/' provides a range of infomation about the process:
				Name:   bash
                Umask:  0022
                State:  S (sleeping)
                Tgid:   17248
                Ngid:   0
                Pid:    17248
                PPid:   17200
                TracerPid:      0
                Uid:    1000    1000    1000    1000
                Gid:    100     100     100     100
                FDSize: 256
                Groups: 16 33 100
                NStgid: 17248
                NSpid:  17248
                NSpgid: 17248
                NSsid:  17200
                VmPeak:     131168 kB
                VmSize:     131168 kB
                VmLck:           0 kB
                VmPin:           0 kB
                VmHWM:       13484 kB
                VmRSS:       13484 kB
                RssAnon:     10264 kB
                RssFile:      3220 kB
                RssShmem:        0 kB
                VmData:      10332 kB
                VmStk:         136 kB
                VmExe:         992 kB
                VmLib:        2104 kB
                VmPTE:          76 kB
                VmPMD:          12 kB
                VmSwap:          0 kB			
				HugetlbPages:          0 kB       
                CoreDumping:   0                 
                Threads:        1
                SigQ:   0/3067
                SigPnd: 0000000000000000
                ShdPnd: 0000000000000000
                SigBlk: 0000000000010000
                SigIgn: 0000000000384004
                SigCgt: 000000004b813efb
                CapInh: 0000000000000000
                CapPrm: 0000000000000000
                CapEff: 0000000000000000
                CapBnd: ffffffffffffffff
                CapAmb:   0000000000000000
                NoNewPrivs:     0
                Seccomp:        0
                Speculation_Store_Bypass:       vulnerable
                Cpus_allowed:   00000001
                Cpus_allowed_list:      0
                Mems_allowed:   1
                Mems_allowed_list:      0
                voluntary_ctxt_switches:        150
                nonvoluntary_ctxt_switches:     545
			
			+ Detailed information: https://man7.org/linux/man-pages/man5/proc.5.html
		
		+ The fact that the contents of this file have changed over time raises a general point about the use of '/proc' file: \
			When these files consist of multiple entries, \
			We should parse them defensively - in this case, looking for a match on a line contain a particular string. \
			(e.g., PPid),\
			rather than processing the file by line number.
			
		+ Lists some other files found in each /proc/PID directory:
			File 				Description (process attribute)
			cmdline 			Command-line arguments delimited by \0
			cwd 				Symbolic link to current working directory
			environ 			Environment list NAME=value pairs, delimited by \0
			exe 				Symbolic link to file being executed
			fd 					Directory containing symbolic links to files opened by this process
			maps 				Memory mappings
			mem 				Process virtual memory (must lseek() to valid offset before I/O)
			mounts 				Mount points for this process
			root 				Symbolic link to root directory
			status 				Various information (e.g., process IDs, credentials, memory usage, signals)
			task 				Contains one subdirectory for each thread in process (Linux 2.6)
		
	12.3. The '/proc/PID/fd' directory:
		+ '/proc/PID/fd' directory contains one symbolic link for each file descriptor that the process has open.
		+ Each of these symbolic links has a name that matches the descriptor number.
		+ AS A CONVENIENCE, any process can access its own /proc/PID/ directory \
			using the symbolic link '/proc/self/'.
			
	12.4. Threads: '/proc/PID/task' directory:		
		+ For each thread in this process, the kernel provides a subdirectory named '/proc/PID/task/TID', \
			where TID is the thread ID of the thread. \
			(This is the same number as would be returned by a call to gettid() in the thread.)
			
		+ Each '/proc/PID/task/TID' subdirectory is a set of files \
			and directories exactly like those that are found under /proc/PID.
			
		+ Since threads share many attributes, much of the information in these files is the same for each of the threads in the process. 
		
	12.5. System information under '/proc/':
		+ Various files and subdirectories under /proc provide access to system-wide information.
			Directory 				Information exposed by files in this directory
			/proc 					Various system information
			/proc/net 				Status information about networking and sockets
			/proc/sys/fs 			Settings related to file systems
			/proc/sys/kernel 		Various general kernel settings
			/proc/sys/net 			Networking and sockets settings
			/proc/sys/vm 			Memory-management settings
			/proc/sys/vipc 			Information about System V IPC objects
			
	12.6. Accessing /proc/ Files:
		+ File under /proc/ are often accessed using shell scripts. \
			(Most /proc files that contain multiple values can be easily parsed with a scripting language such as Python or Perl).
			
		+ '/proc/' files can also be accessed from a program using normal file I/O system calls.
		+ Some restrictions apply when accessing these files:
			+ Some files are read-only.
			+ SOme files can be read only by the file owner.
			+ Other than the files in '/proc/PID/' subdirectories, most files under '/proc/ are owned by root.
			
	12.7. System Identification: uname()
		+ The uname() system call returns a range of identifying information \
			about the host system on which an application is running, in the structure pointed to by utsbuf.
			
			int uname(struct utsname *utsbuf);

			struct utsname {
				char sysname[UTSNAMELENGTH]; 	/* Implementation name */
				char nodename[UTSNAMELENGTH]; 	/* Node name on network */
				char release[UTSNAMELENGTH]; 	/* Implementation release level */
				char version[UTSNAMELENGTH]; 	/* Release version level */
				char machine[UTSNAMELENGTH]; 	/* Hardware on which system is running */
			#ifdef GNUSOURCE 					/* Following is Linux-specific */
				char domainname[UTSNAMELENGTH]; /* NIS domain name of host */
			#endif
			};

	12.8. Summary:
		+ The '/proc/' file system exposes a range of kernel information to application programs.
		+ Each /proc/PID/ subdirectory contains files and subdirectories that \
			provide information about the process whose ID matches PID.
			
		+ Various other files and directories under '/proc/' expose system-wide information that programs
			 can read and, in some cases, modify.
	
	12.9. Further information:
		+ proc(5) manual page: https://man7.org/linux/man-pages/man5/proc.5.html
		+ kernel source file Documentation: https://github.com/torvalds/linux/blob/master/Documentation/filesystems/proc.rst

13. File I/O buffering
	+ In the interests of speed and efficiency, I/O system calls (i.e, the kernel) \
		and the I/O functions of the standard C library (stdio function) buffer data when operating on disk files. \
	+ In this chapter, we describe both types of buffering and consider how they affect application performance.
	+ We also look at various techniques for influencing and disabling both types of buffering, \
		and look at a technique called direct I/O, \
		which is useful for bypassing kernel buffering in certain circumstances.

	13.1. Kernel Buffering of file I/O: buffer cache
		+ When working with disk files, the read() and write() system calls don't directly initiate disk access. \
			Instead, they simply copy data between a user-space buffer and a buffer in the kernel buffer cache. 
			+ For example, the following call transfers 3 bytes of data from a buffer in user-space memory to a buffer in kernel space:
				write(fd, "abc", 3);

		+ At this point, write() returns. At some later point, the kernel writes (flushes) its buffer to the disk. \
			(Hence, we say that the system call is not synchronized with the disk operation.) \
			If, in the interim, another process attempts to read these bytes of the file, \
			then the kernel automatically supplies the data from the buffer cache, \
			rather than from (the outdated contents of) the file.

		+ Correspondingly, for input, the kernel reads data from the disk and stores it in a kernel buffer. \
			Calls to read() fetch data from this buffer until it is exhausted, \
			at which point the kernel reads the next segment of the file into the buffer cache.

		+ The aim of this design is to allow read() and write() to be fast, \
			since they don't need to wait on a (slow) disk operation. \
			This design is also efficient, since it reduces the number of disk transfers that the kernel must perform.
		
		+ The linux kernel imposes no fixed upper limit on the size of the buffer cache. \
			The linux kernel will allocate as many buffer cache pages as are required, \
			limited only by the amount of available physical memory and the demands for physical memory for other purposes \
			(e.g., holding the text and data pages required by running processes).
		+ If available memory is scarce, then the kernel flushes some modified buffer cache pages to disk, \
			in order to free those pages for reuse.

	13.2. Effect of buffer size on I/O system call performance
		+ The kernel performs the same number of disk accesses, \
			regardless of whether we performs 1000 writes of a single byte or a single write of a 1000 bytes. \
			However, the latter is preferable, since it requires a single system call, \
			while the former requires 1000. \
			Although much faster than disk operations, system calls nevertheless amount of time, \
			since the kernel must trap the call, check the validity of the system call arguments, \
			and transfer data between user space and kernel space.

		+ Table 13-1 shows the time that \
			this program requires to copy a file of 100 million bytes on a Linux ext2 file system using different BUF_SIZE values.

		+ Table 13-1. Time required to duplicate a file of 100 million bytes	
			|BUF_SIZE|TIME(second)						   |
			|		 |Elapsed|Total CPU|User CPU|System CPU|
			|--------|-------|---------|--------|----------|
			|1 		 |107.43 |107.32   |8.20    |99.12     |
			|2 		 |54.16  |53.89	   |4.13    |49.76     |
			|4 		 |31.72  |30.96    |2.30    |28.66     |
			|8 		 |15.59  |14.34    |1.08    |13.26     |
			|16 	 |7.50 	 |7.14     |0.51    |6.63      |
			|32 	 |3.76 	 |3.68     |0.26    |3.41      |
			|64 	 |2.19 	 |2.04     |0.13    |1.91      |
			|128 	 |2.16 	 |1.59     |0.11    |1.48      |
			|256 	 |2.06 	 |1.75     |0.10    |1.65      |
			|512 	 |2.06 	 |1.03     |0.05    |0.98      |
			|1024 	 |2.05 	 |0.65     |0.02    |0.63      |
			|4096 	 |2.05 	 |0.38     |0.01    |0.38      |
			|16384 	 |2.05 	 |0.34     |0.00    |0.33      |
			|65536 	 |2.06 	 |0.32     |0.00    |0.32      |
		
		+ Note the following points concerning the information in this table:
			+ The `Elapsed` and `Total CPU` time columns have the obvious meanings. \
				The `User CPU` and `System CPU` columns show a breakdown of the `Total CPU` time into, \
				respectively, the time spent executing code in user code and the time spent executing kernel code (i.e., system calls).
			+ The tests shown in the table were performed using a vanilla 2.6.30 kernel on an `ext2` file system with a block size of 4096 bytes.
			+ Each row shows the average of 20 runs for the given buffer size. \
				In these tests, as in other tests shown later in this chapter, \
				the file system was unmounted and remounted between each execution of the program to ensure that the buffer cache for the file system was empty. \
				Timing was done using the shell `time` command.

		+ Since the total amount of data transferred (and hence the number) is the same for the various buffer sizes, \
			what table 13-1 illustrates is the overhead of making `read()` and `write()` calls. \
			With a buffer size of 1 byte, 100 million calls are made to `read()` and `write()`. \
			With a buffer size of 4096 bytes, \
			the number of invocations of each system call falls to around 24.000, \
			and near optimal performance is reached. \
			Beyond this point, there is no significant performance improvement, \
			because the cost of making read() and write() system calls becomes \
			negligible compared to the time required to copy data between user space and kernel space, \
			and to perform actual disk I/O.

		+ In Summary, if we are transferring a large amount of data to or from a file, \
			then by buffering data in large blocks, \
			and thus performing fewer system calls, \
			we can greatly improve I/O performance.